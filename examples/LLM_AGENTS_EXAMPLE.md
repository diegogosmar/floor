# LLM Agents Example - Complete Walkthrough

This document provides a complete walkthrough of the LLM agent examples, explaining what happens at each step.

## Overview

The `llm_agent_example.py` script demonstrates how to use real LLM providers (OpenAI, Ollama) with Open Floor Protocol agents. Unlike demo agents that use hardcoded responses, these agents generate intelligent responses using actual AI models.

## Running the Example

```bash
# Make sure you have OpenAI API key set
export OPENAI_API_KEY="sk-..."

# Run the examples
python examples/agents/llm_agent_example.py
```

## Example Output Explained

### Example 1: OpenAI GPT-4o-mini Agent

**What happens:**
1. Script checks for `OPENAI_API_KEY` environment variable
2. Creates an `LLMAgent` configured to use OpenAI's `gpt-4o-mini` model
3. Sends a test message: "Hello! Can you help me understand how floor control works?"
4. Agent processes the message through OpenAI API
5. Returns a real AI-generated response

**Output:**
```
âœ… Using OPENAI_API_KEY: sk-xxxxx...
ðŸ“¨ Received message: Hello! Can you help me understand how floor control works?
ðŸ¤– Processing with OpenAI...
ðŸ’¬ Agent response: Sure! Floor control is a method used in discussions...
```

**Key Points:**
- **Response time**: ~4 seconds
- **Response length**: 1564 characters
- **Model**: `gpt-4o-mini` (fast, cost-effective)
- **Real AI**: This is not a hardcoded response - it's generated by GPT-4o-mini

### Example 2: OpenAI GPT-4o Agent

**What happens:**
1. Creates a more capable agent using `gpt-4o` model
2. Asks a more complex question about floor control in multi-agent systems
3. Gets a detailed, technical response

**Output:**
```
âœ… Using OPENAI_API_KEY: sk-xxxxx...
ðŸ“¨ Received message: Explain the concept of floor control in multi-agent systems.
ðŸ¤– Processing with GPT-4o...
ðŸ’¬ Agent response: Floor control in multi-agent systems refers to...
```

**Key Points:**
- **Response time**: ~11 seconds (slower but more detailed)
- **Response length**: 2835 characters (more comprehensive)
- **Model**: `gpt-4o` (more capable, more expensive)
- **Quality**: More detailed technical explanation with examples

### Example 3: Ollama Agent (Local LLM)

**What happens:**
1. Creates an agent using local Ollama instance
2. Uses `llama3.1` model running on your machine
3. No API costs - runs completely locally
4. Requires Ollama to be running: `ollama serve`

**Output:**
```
ðŸ“¨ Received message: Hello! What can you do?
ðŸ¤– Processing with Ollama...
ðŸ’¬ Agent response: I'm excited to help. As a helpful assistant...
```

**Key Points:**
- **Response time**: ~25 seconds (local processing)
- **Response length**: 1045 characters
- **Model**: `llama3.1` (local, free)
- **No API costs**: Runs entirely on your machine
- **Privacy**: All processing happens locally

**Setup Required:**
```bash
# Install Ollama: https://ollama.ai
# Start Ollama server
ollama serve

# Pull the model
ollama pull llama3.1
```

### Example 4: Multi-LLM Agent Conversation

**What happens:**
1. Creates two different OpenAI agents:
   - GPT-4o-mini (fast, economical)
   - GPT-4o (more capable, detailed)
2. First agent asks a simple question
3. Second agent responds with a follow-up based on the first agent's answer
4. Demonstrates conversation history and context awareness

**Output:**
```
ðŸ’¬ GPT-4o-mini Assistant: What is the capital of France?
ðŸ¤– GPT-4o-mini Assistant: The capital of France is Paris.

ðŸ’¬ GPT-4o Assistant: Based on that, what is a famous landmark in Paris?
ðŸ¤– GPT-4o Assistant: A famous landmark in Paris is the Eiffel Tower...
```

**Key Points:**
- **Conversation flow**: Second agent builds on first agent's response
- **Context awareness**: GPT-4o understands "Based on that" refers to Paris
- **Different models**: Shows how different models can collaborate
- **Shared conversation ID**: Both agents use same `conversation_id` to maintain context

## Understanding the Logs

The structured logs show what's happening internally:

```
[info] Processing utterance with LLM
  conversation_id=conv_llm_test
  model=gpt-4o-mini
  provider=openai
  speakerUri=tag:example.com,2025:openai_agent

[info] OpenAI client initialized
  api_key_set=True
  model=gpt-4o-mini

[info] LLM response generated
  response_length=1564
  speakerUri=tag:example.com,2025:openai_agent
```

**What this tells us:**
- Which conversation the message belongs to
- Which model is being used
- How long the response is
- Which agent is processing the message

## Code Structure

### Creating an LLM Agent

```python
from src.agents.llm_agent import LLMAgent

agent = LLMAgent(
    speakerUri="tag:example.com,2025:my_agent",
    agent_name="My Assistant",
    llm_provider="openai",  # or "ollama"
    model_name="gpt-4o-mini",
    system_prompt="You are a helpful assistant."
)
```

### Processing a Message

```python
response = await agent.process_utterance(
    conversation_id="conv_001",
    utterance_text="Hello!",
    sender_speakerUri="tag:example.com,2025:user"
)
```

### Conversation History

The agent automatically maintains conversation history:
- Last 10 messages per conversation
- Separate history for each `conversation_id`
- System prompt included in every request

## Cost Considerations

**OpenAI:**
- GPT-4o-mini: ~$0.15 per 1M input tokens (cheap)
- GPT-4o: ~$2.50 per 1M input tokens (more expensive)

**Ollama:**
- Free (runs locally)
- Requires local GPU/CPU resources

## Troubleshooting

### "OPENAI_API_KEY not set"
```bash
export OPENAI_API_KEY="sk-..."
```

### Ollama 404 Error
```bash
# Make sure Ollama is running
ollama serve

# Pull the model
ollama pull llama3.1
```

### Slow Responses
- GPT-4o-mini: Fast (~4 seconds)
- GPT-4o: Slower (~11 seconds) but more detailed
- Ollama: Slowest (~25 seconds) but free and local

## Next Steps

1. **Customize System Prompts**: Change agent behavior
2. **Use Different Models**: Try `gpt-4`, `gpt-3.5-turbo`, etc.
3. **Integrate with Floor Manager**: Use LLM agents in multi-agent conversations
4. **Add More Providers**: Extend to support other LLM providers

## See Also

- `examples/agents/llm_agent_example.py` - Full source code
- `docs/LLM_INTEGRATION.md` - Complete integration guide
- `src/agents/llm_agent.py` - LLM agent implementation

