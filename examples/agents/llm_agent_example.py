"""
Example: How to use LLM Agent with real LLM providers (OpenAI, Anthropic, Ollama)

This example shows how to create agents that use real LLM models instead of fake echo responses.
"""

import asyncio
import os
import sys

# Add parent directory to path so we can import src
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../.."))

from src.agents.llm_agent import LLMAgent
from src.envelope_router.envelope import (
    OpenFloorEnvelope,
    SchemaObject,
    ConversationObject,
    SenderObject,
    EventObject,
    EventType,
    ToObject
)


async def example_openai_agent():
    """
    Example: Create an agent using OpenAI GPT models
    """
    print("=" * 60)
    print("Example: OpenAI Agent")
    print("=" * 60)

    # OpenAI API key should be set as environment variable
    # The code automatically reads from: $OPENAI_API_KEY
    # Set it with: export OPENAI_API_KEY="sk-..."
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\nâš ï¸  OPENAI_API_KEY not set!")
        print("Please set it: export OPENAI_API_KEY='sk-...'")
        print("Or add to .env file: OPENAI_API_KEY=sk-...")
        return
    
    # Show that we found the key (partial for security)
    key_preview = api_key[:7] + "..." if len(api_key) > 7 else "***"
    print(f"\nâœ… Using OPENAI_API_KEY: {key_preview}")

    # Create OpenAI agent
    agent = LLMAgent(
        speakerUri="tag:example.com,2025:openai_agent",
        agent_name="OpenAI Assistant",
        llm_provider="openai",
        model_name="gpt-4o-mini",  # or "gpt-4", "gpt-3.5-turbo", etc.
        system_prompt="You are a helpful assistant in a multi-agent conversation."
    )

    # Simulate receiving a message
    conversation_id = "conv_llm_test"
    test_message = "Hello! Can you help me understand how floor control works?"

    print(f"\nðŸ“¨ Received message: {test_message}")
    print("\nðŸ¤– Processing with OpenAI...")

    response = await agent.process_utterance(
        conversation_id=conversation_id,
        utterance_text=test_message,
        sender_speakerUri="tag:example.com,2025:user"
    )

    print(f"\nðŸ’¬ Agent response: {response}")

    await agent.stop()


async def example_anthropic_agent():
    """
    Example: Create an agent using Anthropic Claude models
    """
    print("\n" + "=" * 60)
    print("Example: Anthropic Claude Agent")
    print("=" * 60)

    if not os.getenv("ANTHROPIC_API_KEY"):
        print("\nâš ï¸  ANTHROPIC_API_KEY not set!")
        print("Please set it: export ANTHROPIC_API_KEY='sk-ant-...'")
        return

    agent = LLMAgent(
        speakerUri="tag:example.com,2025:claude_agent",
        agent_name="Claude Assistant",
        llm_provider="anthropic",
        model_name="claude-3-haiku-20240307",  # or "claude-3-opus-20240229", etc.
        system_prompt="You are a helpful assistant in a multi-agent conversation."
    )

    conversation_id = "conv_claude_test"
    test_message = "Explain the concept of floor control in multi-agent systems."

    print(f"\nðŸ“¨ Received message: {test_message}")
    print("\nðŸ¤– Processing with Claude...")

    response = await agent.process_utterance(
        conversation_id=conversation_id,
        utterance_text=test_message,
        sender_speakerUri="tag:example.com,2025:user"
    )

    print(f"\nðŸ’¬ Agent response: {response}")

    await agent.stop()


async def example_ollama_agent():
    """
    Example: Create an agent using Ollama (local LLM)
    
    Requires Ollama to be running locally:
    - Install: https://ollama.ai
    - Run: ollama serve
    - Pull model: ollama pull llama3
    """
    print("\n" + "=" * 60)
    print("Example: Ollama Agent (Local LLM)")
    print("=" * 60)

    agent = LLMAgent(
        speakerUri="tag:example.com,2025:ollama_agent",
        agent_name="Local LLM Assistant",
        llm_provider="ollama",
        model_name="llama3",  # or "mistral", "codellama", etc.
        system_prompt="You are a helpful assistant in a multi-agent conversation."
    )

    conversation_id = "conv_ollama_test"
    test_message = "Hello! What can you do?"

    print(f"\nðŸ“¨ Received message: {test_message}")
    print("\nðŸ¤– Processing with Ollama...")
    print("(Make sure Ollama is running: ollama serve)")

    try:
        response = await agent.process_utterance(
            conversation_id=conversation_id,
            utterance_text=test_message,
            sender_speakerUri="tag:example.com,2025:user"
        )
        print(f"\nðŸ’¬ Agent response: {response}")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("Make sure Ollama is running: ollama serve")

    await agent.stop()


async def example_multi_llm_conversation():
    """
    Example: Multiple LLM agents having a conversation
    """
    print("\n" + "=" * 60)
    print("Example: Multi-LLM Agent Conversation")
    print("=" * 60)

    # Check API keys
    has_openai = bool(os.getenv("OPENAI_API_KEY"));
    has_anthropic = bool(os.getenv("ANTHROPIC_API_KEY"));

    if not has_openai and not has_anthropic:
        print("\nâš ï¸  No LLM API keys found!")
        print("Set at least one: OPENAI_API_KEY or ANTHROPIC_API_KEY")
        return

    agents = [];

    if has_openai:
        agent1 = LLMAgent(
            speakerUri="tag:example.com,2025:gpt_agent",
            agent_name="GPT Assistant",
            llm_provider="openai",
            model_name="gpt-4o-mini",
            system_prompt="You are GPT, a helpful assistant. Be concise."
        );
        agents.append(agent1);

    if has_anthropic:
        agent2 = LLMAgent(
            speakerUri="tag:example.com,2025:claude_agent",
            agent_name="Claude Assistant",
            llm_provider="anthropic",
            model_name="claude-3-haiku-20240307",
            system_prompt="You are Claude, a helpful assistant. Be concise."
        );
        agents.append(agent2);

    if not agents:
        return;

    conversation_id = "conv_multi_llm";

    # Agent 1 asks a question
    if agents:
        question = "What is the capital of France?";
        print(f"\nðŸ’¬ {agents[0].agent_name}: {question}");

        response = await agents[0].process_utterance(
            conversation_id=conversation_id,
            utterance_text=question,
            sender_speakerUri="tag:example.com,2025:user"
        );
        print(f"ðŸ¤– {agents[0].agent_name}: {response}");

        # Agent 2 responds (if available)
        if len(agents) > 1:
            follow_up = f"Based on that, what is a famous landmark in {response.split()[-1].rstrip('?.!')}?";
            print(f"\nðŸ’¬ {agents[1].agent_name}: {follow_up}");

            response2 = await agents[1].process_utterance(
                conversation_id=conversation_id,
                utterance_text=follow_up,
                sender_speakerUri=agents[0].speakerUri
            );
            print(f"ðŸ¤– {agents[1].agent_name}: {response2}");

    # Cleanup
    for agent in agents:
        await agent.stop();


async def main():
    """Run examples"""
    print("\nðŸš€ LLM Agent Examples")
    print("=" * 60)
    print("\nThis script demonstrates how to use real LLM providers")
    print("with Open Floor Protocol agents.\n")

    # Example 1: OpenAI
    await example_openai_agent();

    # Example 2: Anthropic
    await example_anthropic_agent();

    # Example 3: Ollama (local)
    await example_ollama_agent();

    # Example 4: Multi-LLM conversation
    await example_multi_llm_conversation();

    print("\n" + "=" * 60)
    print("âœ… Examples completed!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main());

